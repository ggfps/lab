# -*- coding: utf-8 -*-
"""1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/13g8LTY3bnaCQA8U9yFfVuGdp8Kyx_dOH

<a id = 'library'></a>
# Importing libraries and the dataset
"""

import numpy as np # linear algebra
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)

#Lets load the dataset and sample some
column_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'PRICE']
df = pd.read_csv('/content/housing.csv', header=None, delimiter=r"\s+", names=column_names)

from google.colab import drive
drive.mount('/content/drive')

df.head(5)

# Dimension of the dataset
print(np.shape(df))

# Let's summarize the data to see the distribution of data
print(df.describe())

import seaborn as sns
import matplotlib.pyplot as plt

# Plotting boxplots for each feature
sns.boxplot(data=df)
plt.show()



#Looking at the data with names and target variable
df.head()

#Shape of the data
print(df.shape)

#Checking the null values in the dataset
df.isnull().sum()

"""No null values in the dataset, no missing value treatement needed"""

#Checking the statistics of the data
df.describe()

df.info()

"""<a id = 'visual'></a>
# Visualisation
"""

#checking the distribution of the target variable
import seaborn as sns
sns.histplot(df.PRICE , kde = True)

#Distribution using box plot
sns.boxplot(df.PRICE)

#checking Correlation of the data
correlation = df.corr()
correlation.loc['PRICE']

# plotting the heatmap
import matplotlib.pyplot as plt
fig,axes = plt.subplots(figsize=(15,12))
sns.heatmap(correlation,square = True,annot = True)

"""<a id = 'split'></a>
### Splitting the dependent feature and independent feature
"""

#X = data[['LSTAT','RM','PTRATIO']]
X = df.iloc[:,:-1]
y= df.PRICE

"""<a id = 'valid'></a>
### Splitting the data for Model Validation
"""

# Splitting the data into train and test for building the model
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.2, random_state = 4)

"""<a id = 'build'></a>
### Building the Model
"""

#Linear Regression
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()

#Fitting the model
regressor.fit(X_train,y_train)

"""<a id = 'evaluate'></a>
### Model Evaluation
"""

#Prediction on the test dataset
y_pred = regressor.predict(X_test)

print(y_pred)

# Predicting RMSE the Test set results
from sklearn.metrics import mean_squared_error
rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))
print(rmse)

from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print(r2)

"""<a id  = 'NN'></a>
## Neural Networks
"""

#Scaling the dataset
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X_train = sc.fit_transform(X_train)
X_test = sc.transform(X_test)

#Creating the neural network model
import keras
from keras.layers import Dense, Activation,Dropout
from keras.models import Sequential

model = Sequential()

model.add(Dense(128,activation  = 'relu',input_dim =13))
model.add(Dense(64,activation  = 'relu'))
model.add(Dense(32,activation  = 'relu'))
model.add(Dense(16,activation  = 'relu'))
model.add(Dense(1))
model.compile(optimizer = 'adam',loss = 'mean_squared_error')

model.fit(X_train, y_train, epochs = 100)

"""<a id = 'eval'></a>
### Evaluation of the model
"""

y_pred = model.predict(X_test)

from sklearn.metrics import r2_score
r2 = r2_score(y_test, y_pred)
print(r2)

# Predicting RMSE the Test set results
from sklearn.metrics import mean_squared_error
rmse = (np.sqrt(mean_squared_error(y_test, y_pred)))
print(rmse)

"""<a id = 'conclude'></a>
## Conclusion

Using a simple neural network, we were able to improve the model significantly. I encourage you to try alterating the hyperparameters of the model and see if you can get better model
"""